{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMbdNBhLBds7h9rKtVX8+Dp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/islemaiouni/Advanced-Feature-Engineering/blob/main/HR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMs28HqOn9nP"
      },
      "outputs": [],
      "source": [
        "# === Step 1: Create the dataset folder structure ===\n",
        "import os\n",
        "from google.colab import files  # Import files module from google.colab to allow file uploads\n",
        "\n",
        "# Define the base directory in Colab to store the dataset\n",
        "base_dir = \"/content/dataset/dataset-20250309T110448Z-001/dataset\"\n",
        "# Define paths for training and testing directories for each class\n",
        "train_green_dir = os.path.join(base_dir, \"train\", \"green\")\n",
        "train_city_dir  = os.path.join(base_dir, \"train\", \"city\")\n",
        "test_green_dir  = os.path.join(base_dir, \"test\", \"green\")\n",
        "test_city_dir   = os.path.join(base_dir, \"test\", \"city\")\n",
        "\n",
        "# Create directories if they do not already exist\n",
        "os.makedirs(train_green_dir, exist_ok=True)\n",
        "os.makedirs(train_city_dir, exist_ok=True)\n",
        "os.makedirs(test_green_dir, exist_ok=True)\n",
        "os.makedirs(test_city_dir, exist_ok=True)\n",
        "\n",
        "# Print the folder structure to verify it has been created\n",
        "print(\"Folder structure created:\")\n",
        "print(\"Train/green:\", os.listdir(train_green_dir))\n",
        "print(\"Train/city :\", os.listdir(train_city_dir))\n",
        "print(\"Test/green :\", os.listdir(test_green_dir))\n",
        "print(\"Test/city  :\", os.listdir(test_city_dir))\n",
        "\n",
        "# === Step 2: Upload files to each subfolder ===\n",
        "print(\"Please upload files for the train/green folder (e.g., green-001, green-002, …, green-133)\")\n",
        "# Upload files for the train/green folder\n",
        "uploaded_train_green = files.upload()\n",
        "for filename in uploaded_train_green.keys():\n",
        "    # Write each uploaded file into the train/green directory\n",
        "    with open(os.path.join(train_green_dir, filename), 'wb') as f:\n",
        "        f.write(uploaded_train_green[filename])\n",
        "print(\"Files for train/green uploaded.\")\n",
        "\n",
        "print(\"Please upload files for the train/city folder (e.g., city-001, …, city-140)\")\n",
        "# Upload files for the train/city folder\n",
        "uploaded_train_city = files.upload()\n",
        "for filename in uploaded_train_city.keys():\n",
        "    with open(os.path.join(train_city_dir, filename), 'wb') as f:\n",
        "        f.write(uploaded_train_city[filename])\n",
        "print(\"Files for train/city uploaded.\")\n",
        "\n",
        "print(\"Please upload files for the test/green folder (e.g., green-test-001, …, green-test-030)\")\n",
        "# Upload files for the test/green folder\n",
        "uploaded_test_green = files.upload()\n",
        "for filename in uploaded_test_green.keys():\n",
        "    with open(os.path.join(test_green_dir, filename), 'wb') as f:\n",
        "        f.write(uploaded_test_green[filename])\n",
        "print(\"Files for test/green uploaded.\")\n",
        "\n",
        "print(\"Please upload files for the test/city folder (e.g., city-test-001, …, city-test-030)\")\n",
        "# Upload files for the test/city folder\n",
        "uploaded_test_city = files.upload()\n",
        "for filename in uploaded_test_city.keys():\n",
        "    with open(os.path.join(test_city_dir, filename), 'wb') as f:\n",
        "        f.write(uploaded_test_city[filename])\n",
        "print(\"Files for test/city uploaded.\")\n",
        "\n",
        "# Verify the final folder structure by listing the content of each directory\n",
        "print(\"\\nContents of train/green folder:\", os.listdir(train_green_dir))\n",
        "print(\"Contents of train/city folder :\", os.listdir(train_city_dir))\n",
        "print(\"Contents of test/green folder  :\", os.listdir(test_green_dir))\n",
        "print(\"Contents of test/city folder   :\", os.listdir(test_city_dir))\n",
        "\n",
        "# === Step 3: Load the dataset, extract LBP features, and train the models ===\n",
        "import numpy as np\n",
        "import cv2  # Import OpenCV for image processing\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import local_binary_pattern  # Import LBP function from skimage\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold  # For cross-validation\n",
        "from sklearn.svm import SVC  # Support Vector Machine classifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix  # Evaluation metrics\n",
        "from tensorflow.keras.models import Sequential  # For building the ANN model\n",
        "from tensorflow.keras.layers import Dense, Input  # Layers for the ANN model\n",
        "from tensorflow.keras.optimizers import Adam  # Optimizer for training the ANN\n",
        "\n",
        "# Define paths for training and testing data\n",
        "TRAIN_PATH = os.path.join(base_dir, \"train\")\n",
        "TEST_PATH  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "print(\"\\nTraining path:\", TRAIN_PATH)\n",
        "print(\"Test path    :\", TEST_PATH)\n",
        "\n",
        "# Define the categories; these must match the folder names exactly\n",
        "CATEGORIES = [\"green\", \"city\"]\n",
        "\n",
        "# Parameters for LBP feature extraction\n",
        "LBP_RADIUS = 1                   # Radius for LBP calculation\n",
        "LBP_POINTS = 8 * LBP_RADIUS      # Number of sampling points\n",
        "N_BINS = 256                     # Each channel will yield 256 bins; final feature vector length = 256*3 = 768\n",
        "\n",
        "def extract_lbp_histogram(image):\n",
        "    \"\"\"\n",
        "    Extracts the concatenated LBP histogram for the 3 RGB channels of an image.\n",
        "\n",
        "    Parameters:\n",
        "        image: Input RGB image.\n",
        "    Returns:\n",
        "        numpy array: Normalized LBP histogram vector of length 768.\n",
        "    \"\"\"\n",
        "    histogram = []  # Initialize an empty list to store histograms\n",
        "    for i in range(3):  # Loop over each channel (R, G, B)\n",
        "        # Compute the LBP image for the current channel using the 'uniform' method\n",
        "        lbp = local_binary_pattern(image[:, :, i], P=LBP_POINTS, R=LBP_RADIUS, method='uniform')\n",
        "        # Calculate the histogram for LBP values with 256 bins, within the range [0, 256)\n",
        "        hist, _ = np.histogram(lbp.ravel(), bins=N_BINS, range=(0, N_BINS))\n",
        "        # Normalize the histogram so that the sum equals 1\n",
        "        hist = hist.astype(\"float\") / (hist.sum() + 1e-6)\n",
        "        histogram.extend(hist)  # Append the histogram of the current channel to the list\n",
        "    return np.array(histogram)  # Convert the list to a numpy array and return\n",
        "\n",
        "def load_dataset(folder):\n",
        "    \"\"\"\n",
        "    Loads images from a given folder (train or test) and extracts their LBP features.\n",
        "\n",
        "    Parameters:\n",
        "        folder: The path to the dataset folder (either train or test).\n",
        "    Returns:\n",
        "        X: numpy array containing the feature vectors.\n",
        "        y: numpy array containing the corresponding labels.\n",
        "    \"\"\"\n",
        "    X = []  # List to store feature vectors\n",
        "    y = []  # List to store labels\n",
        "    # Iterate over each category and its corresponding label\n",
        "    for label, category in enumerate(CATEGORIES):\n",
        "        category_folder = os.path.join(folder, category)  # Path for the current category\n",
        "        if not os.path.isdir(category_folder):\n",
        "            raise FileNotFoundError(f\"Folder not found: {category_folder}\")\n",
        "        # Loop over each file in the category folder\n",
        "        for file in os.listdir(category_folder):\n",
        "            image_path = os.path.join(category_folder, file)  # Construct the full path to the image\n",
        "            image = cv2.imread(image_path)  # Read the image\n",
        "            if image is None:\n",
        "                continue  # Skip invalid files\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert the image from BGR to RGB\n",
        "            features = extract_lbp_histogram(image)  # Extract LBP features\n",
        "            X.append(features)  # Append feature vector to X\n",
        "            y.append(label)   # Append the corresponding label to y\n",
        "    return np.array(X), np.array(y)  # Convert lists to numpy arrays and return\n",
        "\n",
        "# Load training data and display the number of samples\n",
        "print(\"\\nLoading training data from:\", TRAIN_PATH)\n",
        "X_train, y_train = load_dataset(TRAIN_PATH)\n",
        "print(\"Number of training samples:\", len(y_train))\n",
        "\n",
        "# Load test data and display the number of samples\n",
        "print(\"\\nLoading test data from:\", TEST_PATH)\n",
        "X_test, y_test = load_dataset(TEST_PATH)\n",
        "print(\"Number of test samples:\", len(y_test))\n",
        "\n",
        "# === Train and evaluate the SVM model ===\n",
        "# Initialize an SVM classifier with RBF kernel (change to 'sigmoid' if desired)\n",
        "svm = SVC(kernel='rbf', probability=True)\n",
        "# Set up stratified 5-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# Perform cross-validation on the training data and compute accuracy scores\n",
        "scores_svm = cross_val_score(svm, X_train, y_train, cv=kfold, scoring='accuracy')\n",
        "print(\"\\nAverage SVM accuracy (5-fold): {:.2f}% ± {:.2f}%\".format(scores_svm.mean()*100, scores_svm.std()*100))\n",
        "\n",
        "# Train the SVM model on the entire training set\n",
        "svm.fit(X_train, y_train)\n",
        "# Predict labels for the test set using the trained SVM model\n",
        "svm_predictions = svm.predict(X_test)\n",
        "# Print the classification report and confusion matrix for the SVM model\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"SVM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, svm_predictions))\n",
        "\n",
        "# === Build, train, and evaluate the ANN model ===\n",
        "def build_ann(input_dim):\n",
        "    \"\"\"\n",
        "    Builds an Artificial Neural Network (ANN) with:\n",
        "      - Input layer of dimension 'input_dim'\n",
        "      - Hidden layer with 64 neurons (ReLU activation)\n",
        "      - Hidden layer with 32 neurons (ReLU activation)\n",
        "      - Output layer with 1 neuron (sigmoid activation for binary classification)\n",
        "\n",
        "    Parameters:\n",
        "        input_dim: Number of features in the input.\n",
        "    Returns:\n",
        "        model: A compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),        # Input layer\n",
        "        Dense(64, activation='relu'),       # First hidden layer with 64 neurons\n",
        "        Dense(32, activation='relu'),       # Second hidden layer with 32 neurons\n",
        "        Dense(1, activation='sigmoid')      # Output layer for binary classification\n",
        "    ])\n",
        "    # Compile the model using Adam optimizer and binary crossentropy loss function\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the ANN model using the number of features from the training set\n",
        "ann = build_ann(X_train.shape[1])\n",
        "# Train the ANN model with validation on the test set over 50 epochs and batch size of 32\n",
        "history = ann.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict test labels using the trained ANN model and threshold probabilities at 0.5\n",
        "ann_predictions = (ann.predict(X_test) > 0.5).astype(int)\n",
        "# Print the classification report and confusion matrix for the ANN model\n",
        "print(\"\\nANN Classification Report:\")\n",
        "print(classification_report(y_test, ann_predictions))\n",
        "print(\"ANN Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, ann_predictions))\n",
        "\n",
        "# === Plot the training curves for the ANN model ===\n",
        "plt.figure(figsize=(12, 5))\n",
        "# Plot the training and validation accuracy curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training/Validation Accuracy\")\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training/Validation Loss\")\n",
        "\n",
        "plt.tight_layout()  # Adjust subplots to fit in the figure area\n",
        "plt.show()  # Display the plots\n"
      ]
    }
  ]
}